# This script does three things:  
# 1) It takes the DAMe filtered OTU table and the OTU representative sequences and merges them into an OTU table that has been filtered by taxonomy (here, Arthropoda prob >=0.80 only).  
# 2) Then it carries out a phyloseq analysis of OTU sizes and helps you choose a minimum OTU size, and filters out the smaller OTUs
# 3) Finally, it filters out the small OTUs from the OTU table (OTU x sample) and also creates an R object for downstream community analysis (sample X OTU). 
# 
# 

```{r}
library(vegan); packageVersion("vegan")
library(car)
library(breakaway); packageVersion("breakaway")
library(iNEXT); packageVersion("iNEXT")
library(boral); packageVersion("boral")
library(mvabund); packageVersion("mvabund")
library(PDcalc); packageVersion("PDcalc")
library(tidyverse) # includes all data-formatting packages as one big package (e.g. dplyr, tidyr, ggplot2, readr, readxl, tibble, and others)
# library(dplyr); packageVersion("dplyr")  # for some reason, tidyverse doesn't let me have access to dplyr
library("phyloseq"); packageVersion("phyloseq")
library("data.table"); packageVersion("data.table")
library("ggplot2"); packageVersion("ggplot2")
library("RColorBrewer") # for colours
library("readxl")
library("devtools") 
# devtools::install_github("grunwaldlab/metacoder")
library(metacoder)
sessionInfo()
```


```{r}
# rm(list=ls())
# path_name <- file.path("/Users/Negorashi2011/Xiaoyangmiseqdata/MiSeq_20170410/300Test/analysis/OTUs_min2PCRs_min4copies_2017-11-26_time-1727/OTU_tables/")
path_name <- file.path("/Users/Negorashi2011/Xiaoyangmiseqdata/MiSeq_20170410/300Test/analysis/OTUs_min1PCRs_min1copies_2017-11-30_time-2148/OTU_tables/")
setwd(path_name)
print(path_name)

experiment <- c("A", "B", "C", "D", "E", "F")

taxcolnames <- c("OTU","root","root_tax","root_prob","superkingdom","superkingdom_tax","superkingdom_prob","phylum","phylum_tax","phylum_prob","class","class_tax","class_prob","order","order_tax","order_prob","family","family_tax","family_prob","genus","genus_tax","genus_prob","species","species_tax","species_prob")

# import RDP taxonomies and format. This is the output from RDP Classifier, and it only includes the OTU name (e.g. OTU1) and its Classifier-assigned taxonomies. 
for(i in experiment)
{
	for(j in 97:97)
	{
		assign(paste0("taxonomies_", i, "_", j), setNames(read.table(paste0("table_300test_", i, "_", j, ".RDPmidori_Arthropoda.txt"), header=F, sep = "\t"), taxcolnames))
		# setNames:  sets column names from the taxcolname vector when making (reading) the dataset.
		 assign(paste0("taxonomies_", i, "_", j), dplyr::select(get(paste0("taxonomies_", i, "_", j)), -contains("_tax")))
		# get:  gets the value of the named object. So if it's a dataframe, get will get the dataframe
	}
}

# import DAMe OTU tables. These tables have NOT YET been filtered to include only Arthropoda. 
for(i in experiment)
{
	for(j in 97:97)
	{
		assign(paste0("otutable_", i, "_", j), read.table(paste0("table_300test_", i, "_", j, ".txt"), header=T, sep = "\t"))
		
		assign(paste0("otutable_", i, "_", j), dplyr::mutate(get(paste0("otutable_", i, "_", j)), OTUreadtot=Hhmlbody+hlllbody+hlllleg+Hhmlleg+hhhlleg+hhhlbody+mmmmbody+mmmmleg)) # create new column to sum read numbers over all soups.
		
		assign(paste0("otutable_", i, "_", j), dplyr::select(get(paste0("otutable_", i, "_", j)), OTU,Hhmlbody,hlllbody,hlllleg,Hhmlleg,hhhlleg,hhhlbody,mmmmbody,mmmmleg,PC,OTUreadtot,Seq)) # create OTU table with only these columns
	}
}

```

```{r}
# join the two tables using right_join(), column "OTU" is coerced to char. This filters the OTU table to include only the OTUs that are in the files:  table_300test_A97.RDPmidori_Arthropoda.txt. Thus, only Arthropoda-assigned OTUs are kept. 
for(i in experiment)
{
	for(j in 97:97)
	{
		assign(paste0("otutablefull_", i, "_", j), right_join(get(paste0("otutable_", i, "_", j)), get(paste0("taxonomies_", i, "_", j)), by="OTU"))
	}
}

# # remove the orig OTU and taxonomies files
rm(list = ls(pattern = "otutable_"))
rm(list = ls(pattern = "taxonomies"))

# Now you have taxonomically filtered OTU tables.  

```


#################################
# phyloseq filtering by OTU size. This step removes "small" OTUs, which are probably artefacts of PCR and sequencing error, i.e. echo OTUs, which should have been clustered into the legitimate OTUs. What is "small"?  It is a bit subjective, but the phyloseq method creates a rarefaction curve, showing how many OTUs are at each size class. "Small" OTUs are suggested when there are *very many* OTUs at small sizes. See the graphs created below to understand this.

# Read in an OTU table and convert from OTU X Sample to Sample X OTU
# eval(parse(text = paste0("otutablefull_", experiment, "_", sim))) # to allow dynamic variable names.  It's not useful to make a loop of the following code because one needs to look at the intermediate outputs and make decisions. Luckily, there are only 6 (A-F) to run, per sumaclust similarity percentage (e.g. 97%)


```{r}
experiment <- "F"
sim <- 97

cat("Processing sample ", experiment,"_",sim, "\n", sep="")

communityAll_t <- get(paste0("otutablefull_", experiment, "_", sim)) %>% dplyr::select(one_of(c("OTU","PC", "Hhmlbody","hlllbody","hlllleg","Hhmlleg","hhhlleg","hhhlbody","mmmmbody","mmmmleg")))  # note that in some OTU tables, the order of variables is OTU, PC, then the samples.

# Observe the Positive control OTUs and filter the OTU table.  
communityAll_t <- communityAll_t %>% arrange(desc(PC))
# View(communityAll_t)

# If, for instance, the PC OTUs are only present in the PC sample, then filter out those OTUs. Also, the number of reads per true PC OTU (there are 4 in 300Test) versus the false, smaller, "echo OTUs" gives us guidance on what an artefactual OTU size is (although we generally expect "echo OTUs" in the PC sample to be bigger than echo OTUs in real samples, because there are fewer species in PC samples). 
communityAll_t <- communityAll_t %>% filter(PC <= 0)
communityAll_t <- communityAll_t %>% dplyr::select(-PC) # remove the PC sample
# View(communityAll_t)

#### Now transpose to make canonical OTU tables (sample X OTU format) for community analysis
communityAll <- t(communityAll_t)
colvector <- communityAll[1,] # make a vector of the first row, which has the otu names
communityAll <- as.data.frame(communityAll)
colnames(communityAll) <-  colvector # add the otu names to the column names
communityAll <- communityAll[-1,]
sample_names <- rownames(communityAll)
sample_names.df <- data.frame(sample_names)
# convert the columns to numeric from factor
# http://stackoverflow.com/questions/2288485/how-to-convert-a-data-frame-column-to-numeric-type
communityAll <- sapply(communityAll, function(x) as.numeric(as.character(x))) # sapply applies a function to each column, and the function is:  function(x) as.numeric(as.character(x)).  Cannot convert factors to numeric directly. first convert to character, then to numeric
communityAll <- as.data.frame(communityAll) # then convert to df
# Some OTUs might have 0 total reads. We remove these OTUs if they exist.
communityAll <- communityAll[ , colSums(communityAll)>0]
# View(communityAll)
rm(communityAll_t)


# phyloseq code
TotalCounts <- c(colSums(communityAll))

tdt = data.table(OTUs = colnames(communityAll), TotalCounts = colSums(communityAll), OTU = colnames(communityAll))

ggplot(tdt, aes(TotalCounts)) + 
  geom_histogram() + 
  ggtitle("Histogram of Total Counts per OTU")

taxcumsum = tdt[, .N, by = TotalCounts]
setkey(taxcumsum, TotalCounts)
taxcumsum[, CumSum := cumsum(N)]
# Define the plot
pCumSum = ggplot(taxcumsum, aes(TotalCounts, CumSum)) + 
  geom_point() + 
  xlab("Filtering Threshold:  Minimum Read Number per OTU") +
  ylab("Number of OTUs That Would Be Filtered Out") +
  ggtitle("Number of OTUs that would be filtered out at different minimum OTU read-numbers")
# pCumSum + scale_x_continuous(breaks = scales::pretty_breaks(n = 25)) + scale_y_continuous(breaks = scales::pretty_breaks(n = 25)) # the whole curve. not terribly useful.
# pCumSum + scale_x_continuous(breaks = scales::pretty_breaks(n = 50), limits = c(0, 10000)) + scale_y_continuous(breaks = scales::pretty_breaks(n = 25))
# pCumSum + scale_x_continuous(breaks = scales::pretty_breaks(n = 25), limits = c(0, 500)) + scale_y_continuous(breaks = scales::pretty_breaks(n = 25))
pCumSum + scale_x_continuous(breaks = scales::pretty_breaks(n = 25), limits = c(0, 100)) + scale_y_continuous(breaks = scales::pretty_breaks(n = 25))

cat("There are", dim(communityAll)[2], "OTUs before phyloseq filtering. \n")

threshold_otu_size <- 6
# I look to see if the curve has has an early near-vertical rise, which indicates a large number of small OTUs. For example, in the A_97 dataset, there is not much of one, but there is a bit of a near-vertical rise < OTU size threshold = 12, amounting to ~10 OTUs that would be filtered out (best seen with x-range limit of 0-100). This happens to fit with the PC data, but the point is that there isn't a large number of small OTUs, obviously, **because we already filtered them out via DAMe**. This exercise is subjective, but as a general approach, i choose a threshold number equal to the tangent's x-intercept. This gets rid of the OTUs below the steepest climb in OTU numbers. 
communityAll <- communityAll[, colSums(communityAll) >= threshold_otu_size]
rowSums(communityAll) # confirm that all samples (rows) still have non-zero OTUs in them.  This isn't a risk with this dataset, but some datasets have samples with very few, small OTUs.  Removing small OTUs will produce samples (rows) that have almost no data.  If so, you'll wan to remove these here
cat("There are", dim(communityAll)[2], "OTUs after phyloseq filtering. \n")


#### Create a new full OTU table (OTU X sample), including the taxonomic information and filtering out the phyloseq-determined small OTUs from the original otutablefull_A1_97 type files (using left_join()). This dataset also no longer has the PC column. 
communityAll_t <- t(communityAll)
communityAll_t <- as.data.frame(communityAll_t)
communityAll_t <- rownames_to_column(communityAll_t)
communityAll_t <- communityAll_t %>% rename(OTU=rowname)
# View(communityAll_t)
names(communityAll_t)

assign(paste0("Otutablefull_", experiment, "_", sim, "_minOTU_", threshold_otu_size), left_join(communityAll_t, get(paste0("otutablefull_", experiment, "_", sim)), by="OTU") %>% dplyr::select(-one_of("V1","V2","V3","V4","V5","V6","V7","V8","PC")))

names(get(paste0("Otutablefull_", experiment, "_", sim, "_minOTU_", threshold_otu_size)))
# The filename has the format:  Otutablefull_A_97_minOTU_12. The last number is the minOTU size.

##### Create a list that holds the sample names and the taxonomy and OTU-size-filtered OTU table. 
assign(paste0("Comm_analysis_list_", experiment, "_", sim, "_minOTU_", threshold_otu_size), list(sample_names.df, communityAll))

# The filename has the format:  Comm_analysis_list_A_97_minOTU_12

```

########################################################################
###### Now go back up and redo from experiment <- "A" to do all six experiments
########################################################################



# Congratulations, you now have your final datasets for community analysis. 

# otutablefull_A_96:  The original files from DAMe + RDP Classifier, filtered for taxonomy (typically = Arthropoda at prob ≥ 0.80), in OTU X Sample format, plus RDP Classifier taxonomy)

# Otutablefull_A_96_minOTU_15:  The above file now also filtered for min OTU size, and without the positive control (PC) sample, in OTU X Sample format, plus RDP Classifier taxonomy)

# Comm_analysis_list_A_96_minOTU_15:  The community analysis files in a list:  one with the sample names and one with the OTUs, in Sample X OTU format, no taxonomy)  This is the one that you will likely use the most.  The sample names can be joined with sample information into a larger environment table

```{r write files to disk}
# write otutablefull_A_97 tables to disk
otutablelist <- ls(pattern = "otutablefull") # gets all filenames with "otutablefull" in the name
otutablelist
for(i in 1:length(otutablelist))
{
  write.csv(x = get(otutablelist[i]), file = paste0(otutablelist[i], "_with_tax.csv"), row.names = FALSE)
}

# write Otutablefull_A_97_minOTU_2 tables to disk
Otutablelist <- ls(pattern = "Otutablefull") # gets all filenames with "Otutablefull" in the name
Otutablelist
for(i in 1:length(Otutablelist))
{
  write.csv(x = get(Otutablelist[i]), file = paste0(Otutablelist[i], "_filtered_with_tax.csv"), row.names = FALSE)
}


# save Comm_analysis_list_A_97_minOTU_12 lists to disk, as RDS objects. Can be read in again via readRDS.
commlist <- ls(pattern = "Comm_analysis") # gets all filenames with "Comm_analysis" in the name
commlist
for(i in 1:length(commlist))
{
	saveRDS(get(commlist[i]), file = paste0(commlist[i], ".rds"))
}

```


####################################################################

# To read in one list for analysis
# community <- readRDS("Comm_analysis_list_A_96_minOTU_15.rds")
# env <- community[[1]]
# otus <- community[[2]]

# identical(Comm_analysis_list_A_97_minOTU_12, Comm_analysis_list_A_97_minOTU_12_restored) # identical() checks if the RDS object saved to disk and read back in is the same as the one that was saved. 
# 

########################################################################
#### Community analyses
########################################################################
# Read in all saved community lists for analysis. This works because the min_OTU threshold was the same for all pools. If not the same, then need to use different filenames
#
# rm(list=ls())

# list.files() reads filenames from disk.  edit regex to get differents sets of files

```{r}
# read in community RDS files
experiment <- c("A", "B", "C", "D", "E", "F")
for(i in experiment)
{
  community <- list.files(pattern = paste0("Comm_analysis_list_", i, "_97"))
  assign(paste0("community_", i), readRDS(file = community))
}
```
# identical(community_A, community_D) # confirm that the communities are different.  should be "FALSE"

```{r}
# do NMDS analysis to see basic patterns ####
bodypart <- setNames(as.data.frame(as.vector(c("body", "body", "leg", "leg", "leg", "body", "body", "leg"))), c("bodypart"))
bodypartcol <- setNames(as.data.frame(as.vector(c(1, 1, 2, 2, 2, 1, 1, 2))), c("bodycol"))
evenness <- setNames(as.data.frame(as.vector(c("Hhml", "hlll", "hlll", "Hhml", "hhhl", "hhhl", "mmmm", "mmmm"))), c("evenness"))

# get(paste0("community","A","3"))[[2]] # this syntax works to extract an object from a list that is constructed from a dynamic variable

# NMDS for all communities
for(i in experiment)
{
  env <- get(paste0("community_", i))[[1]]
  env <- bind_cols(env, bodypart, bodypartcol, evenness)
  community <- get(paste0("community_", i))[[2]]
  cat("\n\n", "community_", i, "\n", sep = "")
  community.jmds <- metaMDS(community, distance = "jaccard", trymax = 40, binary=FALSE)
  # community.jmds <- metaMDS(community, distance = "jaccard", binary = FALSE, previous.best = community.jmds)
  assign(paste0("community_", i, ".jmds"), community.jmds)
  rm(community.jmds)
  rm(community)
  rm(env)
}
```

# stressplots 
```{r}
par(mfrow=c(2,3))
for(i in experiment)
{
    stressplot(get(paste0("community_", i, ".jmds")), main = paste0("community", i, ".jmds"))
}
par(mfrow=c(1,1))
```


```{r}
# change the order of the levels in env$bodypart, so that the legend ordered up to down like the points
# RUN ONCE, or the bodypart order will change again
levels(env$bodypart)
env$bodypart <- factor(env$bodypart, levels(env$bodypart)[c(2,1)])
levels(env$bodypart)


(colorvec <- c("#EF8A62", "#67A9CF"))  # from:  brewer.pal(3,"RdBu")
# (colorvec <- c("red2", "mediumblue"))
# http://www.fromthebottomoftheheap.net/2012/04/11/customising-vegans-ordination-plots/
# https://github.com/hallamlab/mp_tutorial/wiki/Taxonomic-Analysis
```
# with(env, colorvec[bodypart]) # method to use the bodypart levels (1 and 2) to set the colors from colorvec)

```{r}
# ordinationplot function
ordinationplot <- function(lib, env) {
  ## extract scrs
  sites <- scores(get(lib), display = "sites")
  spps  <- scores(get(lib), display = "species")
  
  ## compute axis ranges from scrs
  xlim <- range(sites[,1], spps[,1])
  ylim <- range(sites[,2], spps[,2])
  
  # colorvec <- c("red2", "mediumblue")
  plot(get(lib), ylab="", xlab="", xlim=xlim, ylim=ylim, type="n", scaling = 3, main = lib)
  points(get(lib), display = "sites", pch=16, cex=sprichness/30, col=colorvec[env$bodypart])
  # points(get(lib), display = "sites", pch=16, cex=2, col=colorvec[env$bodypart])
  
  with(env, legend("top", legend = levels(bodypart), bty = "n", col=colorvec, pt.cex=2, pch=16))
  cexnum <- 0.5
  # text(sites, labels = env$sample_names, col = "black", cex = 0.6)
  with(env, ordispider(get(lib), evenness, cex=cexnum, draw="polygon", col=c("black"), alpha=100, kind="se", conf=0.95, label=TRUE, 
                       show.groups=(c("hlll"))))
  with(env, ordispider(get(lib), evenness, cex=cexnum, draw="polygon", col=c("black"), alpha=100, kind="se", conf=0.95, label=TRUE,
                       show.groups=(c("Hhml"))))
  with(env, ordispider(get(lib), evenness, cex=cexnum, draw="polygon", col=c("black"), alpha=100, kind="se", conf=0.95, label=TRUE,
                       show.groups=(c("hhhl"))))
  with(env, ordispider(get(lib), evenness, cex=cexnum, draw="polygon", col=c("black"), alpha=100, kind="se", conf=0.95, label=TRUE,
                       show.groups=(c("mmmm"))))
}
```

# **compare the different experiments**
```{r}
# plot all the ordinations
experiment <- c("A", "B", "C", "D", "E", "F")
par(mfrow=c(3,2))
for(i in experiment)
{
  exptindex <- i
  ord <- ".jmds" # ".jmds, .ca, .pca" are the options
  expt <- paste0("community_", exptindex, ord)
  cat("Active ordination is:", expt, "\n")
  
  community <- get(paste0("community_", i))[[2]]
  sprichness <- specnumber(community, MARGIN = 1) # calc species richnesses for each experiment
  
  ordinationplot(expt, env)

  rm(community)
  rm(sprichness)
}
par(mfrow=c(1,1))

```

# plot an ordination from just one experiment
```{r eval=FALSE, include=FALSE}

experiment <- c("A")

par(mfrow=c(1,1))
for(i in experiment)
{
  exptindex <- i
  ord <- ".jmds" # ".jmds, .ca, .pca" are the options
  expt <- paste0("community_", exptindex, ord)
  cat("Active ordination is:", expt, "\n")
  
  community <- get(paste0("community_", i))[[2]]
  sprichness <- specnumber(community, MARGIN = 1) # calc species richnesses for each experiment
  
  ordinationplot(expt, env)

  rm(community)
  rm(sprichness)
}
par(mfrow=c(1,1))

```

# calculate species richness in each Experiment
```{r}
for(i in experiment)
{
  env <- get(paste0("community_", i))[[1]]
  env <- bind_cols(env, bodypart, bodypartcol, evenness)
  community <- get(paste0("community_", i))[[2]]
  cat(print(levels(env$sample_names)), "\n")
  cat("Experiment ", i, ":  species richness", "\n", sep = "")
  sprichness <- specnumber(community, groups = env$sample_names, MARGIN = 1) # number of species per site. NB can't calculate Chao2 because each sample has n=1.  
  cat(sprichness, "\n")
  cat("Experiment ", i, ":  Read totals", "\n", sep = "")
  cat(rowSums(community), "\n\n")
  rm(community)
}
```

# procrustes analyses
```{r}
protestAB <- protest(community_A.jmds, community_B.jmds)
protestAC <- protest(community_A.jmds, community_C.jmds)
protestAD <- protest(community_A.jmds, community_D.jmds)
protestAE <- protest(community_A.jmds, community_E.jmds)
protestAF <- protest(community_A.jmds, community_F.jmds)
protestBC <- protest(community_B.jmds, community_C.jmds)
protestBD <- protest(community_B.jmds, community_D.jmds)
protestBE <- protest(community_B.jmds, community_E.jmds)
protestBF <- protest(community_B.jmds, community_F.jmds)
protestCD <- protest(community_C.jmds, community_D.jmds)
protestCE <- protest(community_C.jmds, community_E.jmds)
protestCF <- protest(community_C.jmds, community_F.jmds)
protestDE <- protest(community_D.jmds, community_E.jmds)
protestDF <- protest(community_D.jmds, community_F.jmds)
protestEF <- protest(community_E.jmds, community_F.jmds)

# plot the procrustes superimposition graphs.
# Top row Procrustes are between libraries using the same tags. The rest of the rows are between libraries using different tags.
par(mfrow=c(4,4))
plot(protestAB, main = "A vs B")
plot(protestAC, main = "A vs C")
plot(protestAD, main = "A vs D")
plot(protestAE, main = "A vs E")
plot(protestAF, main = "A vs F")
plot(protestBC, main = "B vs C")
plot(protestBD, main = "B vs D")
plot(protestBE, main = "B vs E")
plot(protestBF, main = "B vs F")
plot(protestCD, main = "C vs D")
plot(protestCE, main = "C vs E")
plot(protestCF, main = "C vs F")
plot(protestDE, main = "D vs E")
plot(protestDF, main = "D vs F")
plot(protestEF, main = "E vs F")
par(mfrow=c(1,1))

combinations <- combn(experiment, 2)
combinations
# store correlation coefficients from Protest, omitting ones with A2
pairwise <- c("AB", "AC", "AD", "AE", "AF", "BC", "BD", "BE", "BF", "CD", "CE", "CF", "DE", "DF", "EF")

# store correlation coefficients from Protest
correlations <- 0
j=0
for (i in pairwise) {
  j=j+1
  correlations[j] <- get(paste0("protest", i))[["scale"]] # correlation coefficient from protest 
}

correlations
length(correlations)
mean(correlations)
sd(correlations)/sqrt(length(correlations))


```


########################################################################
Dropout analyses
########################################################################

```{r}
path_name <- file.path("/Users/Negorashi2011/Xiaoyangmiseqdata/MiSeq_20170410/300Test/analysis/OTUs_min2PCRs_min4copies_2017-11-26_time-1727/OTU_tables/")
setwd(path_name)
print(path_name)

mtbcolnames <- c("qseqid", "sseqid", "pident", "length", "mismatch", "gapopen", "qstart", "qend", "sstart", "send", "evalue", "bitscore")

experiment <- c("A", "B", "C", "D", "E", "F")

# read in the BLAST (OTU to MTB) output file
for(i in experiment)
{
    community <- list.files(pattern = paste0("table_300test_", i, "_97_Arthropoda.blastnMTB.txt"))
    assign(paste0("community_MTB_", i), setNames(read.table(file = community), mtbcolnames))
    assign(paste0("community_MTB_", i), dplyr::filter(get(paste0("community_MTB_", i)), pident >=97.5))  
}
# if i use pident >=97, there is one MTB that gets matched to two OTUs (79 and 265). 265 is a 97.143% pident match to MTB237. so i have set min pident to 97.5. really, i need to run lulu instead of this

# read in the OTU table:  this one no longer has PC OTUs in it
for(i in experiment)
{
    community <- list.files(pattern = paste0("Otutablefull_", i, "_97_minOTU_6_filtered_with_tax.csv"))
    assign(paste0("Otutablefull_", i, "_97_minOTU_6_filtered_with_tax"), read_csv(file = community))
}

# join the OTU and BLASTMTB tables
for(i in experiment)
{
    assign(paste0("otuMTB_", i), left_join(get(paste0("Otutablefull_", i, "_97_minOTU_6_filtered_with_tax")), get(paste0("community_MTB_", i)), by= c("OTU" = "qseqid")))
}

# read in MTB_REF file (updated on 20171129 to include the input DNA amounts and morphological ID to order level)
excel_path_name <- file.path("/Users/Negorashi2011/Xiaoyangmiseqdata/MiSeq_20170410/300Test/data/MTB/MTB_AllInputRefSeqs_20171130.xlsx")
MTB_excel <- read_excel(path = excel_path_name, sheet = "RDP_midori", col_types = c("text","numeric","numeric","numeric","numeric","numeric","numeric","numeric","numeric","text","text","logical","text","text","skip","skip","skip","skip","skip","skip","skip","skip","skip","skip","skip","skip","skip","skip","skip","skip"))
# MTB_excel <- MTB_excel %>% dplyr::select(MTBseq:RDP_concatenated)

# join the OTU_BLASTMTB and MTB_REF files, remove PC OTUs, and sort by OTUreadtot
for(i in experiment)
{
    assign(paste0("otu_MTB_REF_", i), left_join(MTB_excel, get(paste0("otuMTB_", i)), by= c("MTBseq" = "sseqid")))
    # filter out the PC OTUs
    assign(paste0("otu_MTB_REF_", i), dplyr::filter(get(paste0("otu_MTB_REF_", i)), !MTBseq %in% c("P1", "P2", "P4", "P8") )) 
    # replace the NAs in the OTU read number columns with 0s (see stackoverflow post below)
    assign(paste0("otu_MTB_REF_", i), dplyr::mutate_at(get(paste0("otu_MTB_REF_", i)), vars(Hhmlbody:OTUreadtot), funs(replace(., is.na(.), 0)))) 
    assign(paste0("otu_MTB_REF_", i), dplyr::arrange(get(paste0("otu_MTB_REF_", i)), desc(OTUreadtot) )) 
    # communityAll_t <- communityAll_t %>% arrange(desc(PC))
}

```

great post on how to change a variable (using various versions of dplyr::mutate)
https://stackoverflow.com/questions/8161836/how-do-i-replace-na-values-with-zeros-in-an-r-dataframe


```{r}
# make summary table
for(i in experiment)
{ 
    assign(paste0("otu_MTB_REF_summ", i),
    get(paste0("otu_MTB_REF_", i)) %>% summarise(
    hlllleg = sum(hlllleg > 0),
    hlllbody = sum(hlllbody > 0),
    Hhmlleg = sum(Hhmlleg > 0),
    Hhmlbody = sum(Hhmlbody > 0),
    hhhlleg = sum(hhhlleg > 0),
    hhhlbody = sum(hhhlbody > 0),
    mmmmleg = sum(mmmmleg > 0),
    mmmmbody = sum(mmmmbody > 0)) )
}

# if there are NAs in the columns (but i removed NAs in the previous chunk)
# for(i in experiment)
# { 
#     assign(paste0("otu_MTB_REF_summ", i),
#     get(paste0("otu_MTB_REF_", i)) %>% summarise(
#     hlllleg = sum(hlllleg > 0, na.rm = TRUE),
#     hlllbody = sum(hlllbody > 0, na.rm = TRUE),
#     Hhmlleg = sum(Hhmlleg > 0, na.rm = TRUE),
#     Hhmlbody = sum(Hhmlbody > 0, na.rm = TRUE),
#     hhhlleg = sum(hhhlleg > 0, na.rm = TRUE),
#     hhhlbody = sum(hhhlbody > 0, na.rm = TRUE),
#     mmmmleg = sum(mmmmleg > 0, na.rm = TRUE),
#     mmmmbody = sum(mmmmbody > 0, na.rm = TRUE)) )
# }
# na.rm = T needed because NAs cause NAs in the output

for(i in experiment)
{ 
    assign(paste0("otu_MTB_REF_summ", i),
    get(paste0("otu_MTB_REF_summ", i)) %>% 
    mutate(experiment = i) %>% 
    mutate(maxMTB = nrow(get(paste0("otu_MTB_REF_", i))))) # number of MTB seqs
}

otu_MTB_REF_summ_all <- NULL
for(i in experiment)
{ 
    otu_MTB_REF_summ_all <- bind_rows(otu_MTB_REF_summ_all, get(paste0("otu_MTB_REF_summ", i)))
}

otu_MTB_REF_summ_all <- otu_MTB_REF_summ_all %>% dplyr::select(experiment, hlllleg:mmmmbody, maxMTB)
otu_MTB_REF_summ_pct <- otu_MTB_REF_summ_all %>% mutate_at(vars(hlllleg:mmmmbody), funs(./maxMTB)) %>% mutate_at(vars(hlllleg:mmmmbody), funs(round(., digits = 2)))
```

```{r eval=FALSE, include=FALSE}
write.table(otu_MTB_REF_summ_all, file = "otu_MTB_REF_summ_all.txt")
write.table(otu_MTB_REF_summ_pct, file = "otu_MTB_REF_summ_pct.txt")
```

Read number vs. input DNA concentrations.
Concentration: ng/ul are the units for input DNA (e.g. hlll_body_DNA), measured via qPCR (thus measuring COI amplicons.  the scale is relative because used tissue DNA as a standard curve)
PCR input 5ul mix DNA
```{r}
df <- otu_MTB_REF_F
par(mfrow=c(4,2))
plot(hlllbody ~ hlll_body_DNA, data = df, xlim=c(0,250), ylim=c(0,8000))
plot(Hhmlbody ~ Hhml_body_DNA, data = df, xlim=c(0,250), ylim=c(0,8000))
plot(hhhlbody ~ hhhl_body_DNA, data = df, xlim=c(0,100), ylim=c(0,8000))
plot(mmmmbody ~ mmmm_body_DNA, data = df, xlim=c(0,10), ylim=c(0,8000))
# par(mfrow=c(1,1))

# par(mfrow=c(2,2))
plot(hlllleg ~ hlll_leg_DNA, data = df, xlim=c(0,4), ylim=c(0,6000))
plot(Hhmlleg ~ Hhml_leg_DNA, data = df, xlim=c(0,80), ylim=c(0,6000))
plot(hhhlleg ~ hhhl_leg_DNA, data = df, xlim=c(0,4), ylim=c(0,6000))
plot(mmmmleg ~ mmmm_leg_DNA, data = df, xlim=c(0,.2), ylim=c(0,6000))
par(mfrow=c(1,1))
```




Example code for dplyr::summarise
```{r eval=FALSE, include=FALSE}
library(nycflights13)
summarise(flights, delay = mean(dep_delay, na.rm = TRUE))
by_day <- group_by(flights, year, month, day)
summarise(by_day, delay = mean(dep_delay, na.rm = TRUE))

by_dest <- group_by(flights, dest)
delay <- summarise(by_dest,
  count = n(),
  dist = mean(distance, na.rm = TRUE),
  delay = mean(arr_delay, na.rm = TRUE)
)
delay <- filter(delay, count > 20, dest != "HNL")


not_cancelled <- flights %>% 
  filter(!is.na(dep_delay), !is.na(arr_delay))

delays <- not_cancelled %>% 
  group_by(tailnum) %>% 
  summarise(
    delay = mean(arr_delay, na.rm = TRUE),
    n = n()
  )

ggplot(data = delays, mapping = aes(x = n, y = delay)) + 
  geom_point(alpha = 1/10)

```

